import numpy as np
from scipy.stats import multivariate_normal     # å¼•å…¥å¤šå…ƒæ­£æ€åˆ†å¸ƒè®¡ç®—å‡½æ•°
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# æ•°æ®åŠ è½½   è¿™é‡ŒåŠ è½½æ—¶ï¼Œå‰10ä¸ªæ ·æœ¬æ˜¯ç±»åˆ«1ï¼Œä¸­é—´10ä¸ªæ˜¯ç±»åˆ«2ï¼Œæœ€åæ˜¯ç±»åˆ«3ï¼Œæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬ç‚¹çš„ä¸‰ä¸ªç‰¹å¾å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­
data = np.array([
    [-5.01, -8.12, -3.68], [-5.43, -3.48, -3.54], [1.08, -5.52, 1.66],
    [0.86, -3.78, -4.11], [-2.67, 0.63, 7.39], [4.94, 3.29, 2.08],
    [-2.51, 2.09, -2.59], [-2.25, -2.13, -6.94], [5.56, 2.86, -2.26],
    [1.03, -3.33, 4.33], [-0.91, -0.18, -0.05], [1.30, -2.06, -3.53],
    [-7.75, -4.54, -0.95], [-5.47, 0.50, 3.92], [6.14, 5.72, -4.85],
    [3.60, 1.26, 4.36], [5.37, -4.63, -3.65], [7.18, 1.46, -6.66],
    [-7.39, 1.17, 6.30], [-7.50, -6.32, -0.31], [5.35, 2.26, 8.13],
    [5.12, 3.22, -2.66], [-1.34, -5.31, -9.87], [4.48, 3.42, 5.19],
    [7.11, 2.39, 9.21], [7.17, 4.33, -0.98], [5.75, 3.97, 6.65],
    [0.77, 0.27, 2.41], [0.90, -0.43, -8.71], [3.52, -0.36, 6.43]
])

# å°†æ•°æ®åˆ†ä¸ºä¸‰ç±»
class_1, class_2, class_3 = data[:10], data[10:20], data[20:]    ##å‰10ä¸ªæ ·æœ¬ä¸ºğœ”1ï¼Œä¸­é—´10ä¸ªæ˜¯ğœ”2ï¼Œæœ€åæ˜¯Ï‰3

def estimate_parameters(X):
    """
    å‚æ•°ä¼°è®¡
    @param X: æ ·æœ¬æ•°æ®
    @return: å‡å€¼å’Œåæ–¹å·®çŸ©é˜µ
    """
    return np.mean(X, axis=0), np.cov(X, rowvar=False)       # è¿”å›è¾“å…¥æ ·æœ¬çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µ rowvar=Falseè¡¨ç¤ºæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾  è¾“å…¥çš„Xå¯ä»¥æ˜¯ä¸€ç»´ã€äºŒç»´å’Œä¸‰ç»´æ•°ç»„

def calculate_error_rate(true_labels, predicted_labels):
    """
    è®¡ç®—åˆ†ç±»é”™è¯¯ç‡
    @param true_labels: çœŸå®æ ‡ç­¾
    @param predicted_labels: é¢„æµ‹æ ‡ç­¾
    @return: é”™è¯¯ç‡  è¿™ä¸ªé”™è¯¯ç‡å°±æ˜¯åˆ†ç±»é”™è¯¯çš„æ ·æœ¬æ•°é™¤ä»¥æ€»çš„æ ·æœ¬æ•°
    """
    return np.mean(np.array(true_labels) != np.array(predicted_labels))  # è®¡ç®—é”™è¯¯ç‡ np.array(true_labels) != np.array(predicted_labels)è¿”å›ä¸€ä¸ªå¸ƒå°”æ•°ç»„ï¼ŒTrueè¡¨ç¤ºåˆ†ç±»é”™è¯¯ï¼ŒFalseè¡¨ç¤ºåˆ†ç±»æ­£ç¡®  np.mean()è®¡ç®—Trueçš„æ¯”ä¾‹

def bayes_classifier(x, means, covs, priors):
    """
    è´å¶æ–¯åˆ†ç±»å™¨  
    è¾“å…¥ä¸€ä¸ªæ ·æœ¬ç‚¹ç‰¹å¾ï¼Œä¹‹åè®¡ç®—è¿™ä¸ªæ ·æœ¬ç‚¹å±äºæ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡ï¼ˆæ¡ä»¶æ¦‚ç‡Ã—å…ˆéªŒæ¦‚ç‡ï¼‰ï¼Œè¿”å›æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«çš„æ ‡ç­¾
    @param x: æ ·æœ¬ç‚¹ï¼Œå³ç‰¹å¾å‘é‡ï¼Œå¯ä»¥æ˜¯ä¸€ç»´ã€äºŒç»´æˆ–ä¸‰ç»´
    @param means: å„ç±»åˆ«çš„å‡å€¼å‘é‡
    @param covs: å„ç±»åˆ«çš„åæ–¹å·®çŸ©é˜µ
    @param priors: å„ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡
    @return: é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾
    """
    probs = [multivariate_normal.pdf(x, mean=mean, cov=cov) * prior 
             for mean, cov, prior in zip(means, covs, priors)]       # ç”¨å¤šå…ƒæ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡  probsåˆ—è¡¨åŒ…å«æ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡
    return np.argmax(probs) + 1  # è¿”å›æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«çš„æ ‡ç­¾  np.argmax(probs)è¿”å›çš„æ˜¯ probs åˆ—è¡¨ä¸­æœ€å¤§å€¼çš„ç´¢å¼•  +1åï¼Œç±»åˆ«æ ‡ç­¾è¢«å®šä¸º1ã€2æˆ–è€…1ã€2ã€3

def classify_samples(X, means, covs, priors):
    """
    å¯¹å¤šä¸ªæ ·æœ¬è¿›è¡Œåˆ†ç±»
    @param X: æ€»çš„æ ·æœ¬æ•°æ®
    @param means: å„ç±»åˆ«çš„å‡å€¼å‘é‡
    @param covs: å„ç±»åˆ«çš„åæ–¹å·®çŸ©é˜µ
    @param priors: å„ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡
    @return: é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾æ•°ç»„
    """
    return np.array([bayes_classifier(x, means, covs, priors) for x in X])  # ä»Xä¸­å–å‡ºæ¯ä¸ªæ ·æœ¬ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œåˆ†ç±»   è¿”å›ä¸€ä¸ªæ•°ç»„ï¼Œæ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾

# ç»˜å›¾
class Visualizer:
    """
    å¯è§†åŒ–ç±»ï¼Œç”¨äºç»˜åˆ¶å†³ç­–è¾¹ç•Œå’Œé”™è¯¯ç‡
    """
    @staticmethod
    def plot_decision_boundary(X, y, means, covs, priors, error_rate, dim):
        """
        ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        """
        plt.figure(figsize=(10, 6) if dim == 1 else (10, 8))  # è®¾ç½®å›¾åƒå¤§å°
        colors = ['r', 'b', 'g']  # é¢œè‰²
        labels = ['ç±»åˆ«1', 'ç±»åˆ«2', 'ç±»åˆ«3']  # æ ‡ç­¾
        
        y = np.array(y)
        
        if dim == 1:
            Visualizer._plot_1d(X, y, means, covs, priors, colors, labels)
        else:
            Visualizer._plot_2d(X, y, means, covs, priors, colors, labels)
        
        plt.title(f"{'äºŒ' if len(means) == 2 else 'ä¸‰'}ç±»é—®é¢˜ï¼š{dim}ç»´ç‰¹å¾çš„å†³ç­–è¾¹ç•Œ (é”™è¯¯ç‡: {error_rate:.2%})")
        plt.xlabel("x1")
        plt.ylabel("x2" if dim > 1 else "")
        plt.legend()
        plt.show()

    @staticmethod
    def _plot_1d(X, y, means, covs, priors, colors, labels):
        """
        ç»˜åˆ¶ä¸€ç»´å†³ç­–è¾¹ç•Œ
        """
        for i, (color, label) in enumerate(zip(colors, labels)):
            class_data = X[y == i+1]
            plt.scatter(class_data, np.zeros_like(class_data), c=color, label=label)
        
        x_range = np.linspace(X.min() - 1, X.max() + 1, 1000).reshape(-1, 1)
        y_pred = classify_samples(x_range, means, covs, priors)
        
        for i, color in enumerate(colors):
            plt.plot(x_range[y_pred == i+1], [(i-1)*0.1] * np.sum(y_pred == i+1), f'{color}.', alpha=0.1)
        
        plt.ylim(-0.5, 0.5)
        plt.yticks([])

    @staticmethod
    def _plot_2d(X, y, means, covs, priors, colors, labels):
        """
        ç»˜åˆ¶äºŒç»´å†³ç­–è¾¹ç•Œ
        """
        for i, (color, label) in enumerate(zip(colors, labels)):
            class_data = X[y == i+1]
            plt.scatter(class_data[:, 0], class_data[:, 1], c=color, label=label)
        
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
        Z = classify_samples(np.c_[xx.ravel(), yy.ravel()], means, covs, priors)
        Z = Z.reshape(xx.shape)
        
        cmap = plt.cm.RdYlBu if len(means) == 2 else plt.cm.viridis
        plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap)
        plt.contour(xx, yy, Z, colors='k', linewidths=0.5)

    @staticmethod
    def plot_decision_boundary_3d(X, y, means, covs, priors, error_rate, class_count):
        """
        ç»˜åˆ¶ä¸‰ç»´å†³ç­–è¾¹ç•Œ
        """
        fig = plt.figure(figsize=(12, 8))
        ax = fig.add_subplot(111, projection='3d')
        colors = ['r', 'b', 'g']
        labels = ['ç±»åˆ«1', 'ç±»åˆ«2', 'ç±»åˆ«3'][:class_count]
        
        y = np.array(y)
        
        for i, (color, label) in enumerate(zip(colors[:class_count], labels)):
            class_data = X[y == i+1]
            ax.scatter(class_data[:, 0], class_data[:, 1], class_data[:, 2], c=color, label=label)
        
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        z_min, z_max = X[:, 2].min() - 1, X[:, 2].max() + 1
        xx, yy, zz = np.meshgrid(np.linspace(x_min, x_max, 25),
                                 np.linspace(y_min, y_max, 25),
                                 np.linspace(z_min, z_max, 25))
        
        grid = np.c_[xx.ravel(), yy.ravel(), zz.ravel()]
        Z = classify_samples(grid, means, covs, priors)
        Z = Z.reshape(xx.shape)
        
        cmap = plt.cm.RdYlBu if class_count == 2 else plt.cm.viridis
        scatter = ax.scatter(xx, yy, zz, c=Z.ravel(), cmap=cmap, alpha=0.2, s=10)
        
        plt.colorbar(scatter, ax=ax, label='ç±»åˆ«')
        
        ax.set_title(f"{'äºŒ' if class_count == 2 else 'ä¸‰'}ç±»é—®é¢˜ï¼šä¸‰ç»´ç‰¹å¾çš„å†³ç­–è¾¹ç•Œ (é”™è¯¯ç‡: {error_rate:.2%})")
        ax.set_xlabel("x1")
        ax.set_ylabel("x2")
        ax.set_zlabel("x3")
        ax.legend()
        plt.show()

    @staticmethod
    def plot_error_rates(error_rates):
        """
        ç»˜åˆ¶ä¸åŒç»´åº¦ç‰¹å¾çš„é”™è¯¯ç‡æŸ±çŠ¶å›¾
        """
        plt.figure(figsize=(10, 6))
        plt.bar(['1D', '2D', '3D'], error_rates)
        plt.title("ä¸åŒç»´åº¦ç‰¹å¾çš„é”™è¯¯ç‡")
        plt.xlabel("ç‰¹å¾ç»´åº¦")
        plt.ylabel("é”™è¯¯ç‡")
        for i, v in enumerate(error_rates):
            plt.text(i, v, f'{v:.2%}', ha='center', va='bottom')
        plt.show()

def run_classification(classes, dims, priors):
    """
    è¿è¡Œåˆ†ç±»å®éªŒ
    @param classes: åŒ…å«å„ä¸ªç±»åˆ«æ•°æ®çš„åˆ—è¡¨
    @param dims: è¦æµ‹è¯•çš„ç‰¹å¾ç»´åº¦åˆ—è¡¨
    @param priors: å„ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡
    """
    error_rates = []  # é”™è¯¯ç‡åˆ—è¡¨
    for dim in dims:
        X = np.vstack([cls[:, :dim] for cls in classes])  # å°†æ‰€æœ‰ç±»åˆ«çš„æ•°æ®åˆå¹¶ä¸ºä¸€ä¸ªæ•°ç»„  ï¼ˆæ ¹æ®dimå–å‰dimä¸ªç‰¹å¾ï¼‰
        true_labels = [i+1 for i, cls in enumerate(classes) for _ in range(len(cls))]  # çœŸå®æ ‡ç­¾åˆ—è¡¨ 10ä¸ª1ï¼Œ10ä¸ª2ï¼Œ10ä¸ª3
        means = [estimate_parameters(cls[:, :dim])[0] for cls in classes]  # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡å€¼å‘é‡ ï¼ˆæ ¹æ®dimå–å‰dimä¸ªç‰¹å¾ï¼‰
        covs = [estimate_parameters(cls[:, :dim])[1] for cls in classes]  # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åæ–¹å·®çŸ©é˜µ ï¼ˆæ ¹æ®dimå–å‰dimä¸ªç‰¹å¾ï¼‰
        
        # æ ¹æ®ä¸åŒç»´åº¦ç‰¹å¾çš„åˆ†ç±»å™¨å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»
        predicted_labels = classify_samples(X, means, covs, priors)  # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œåˆ†ç±»   è¿”å›æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾
        error_rate = calculate_error_rate(true_labels, predicted_labels)  # è®¡ç®—é”™è¯¯ç‡
        error_rates.append(error_rate)  # å°†é”™è¯¯ç‡æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        
        print(f"{'äºŒ' if len(classes) == 2 else 'ä¸‰'}ç±»é—®é¢˜ä½¿ç”¨{dim}ç»´ç‰¹å¾çš„åˆ†ç±»å™¨é”™è¯¯ç‡: {error_rate:.2%}")  # æ‰“å°é”™è¯¯ç‡
        if dim <= 2:
            Visualizer.plot_decision_boundary(X, true_labels, means, covs, priors, error_rate, dim)  # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        elif dim == 3:
            Visualizer.plot_decision_boundary_3d(X, true_labels, means, covs, priors, error_rate, len(classes))  # ç»˜åˆ¶ä¸‰ç»´å†³ç­–è¾¹ç•Œ
    
    Visualizer.plot_error_rates(error_rates)  # ç»˜åˆ¶é”™è¯¯ç‡æŸ±çŠ¶å›¾

def main():
    # äºŒç±»é—®é¢˜
    run_classification([class_1, class_2], [1, 2, 3], [0.5, 0.5])  # ä¼ å…¥ä¸¤ç±»æ•°æ®ï¼Œ3ä¸ªç»´åº¦ç‰¹å¾ï¼Œå…ˆéªŒæ¦‚ç‡ä¸º0.5
    
    # ä¸‰ç±»é—®é¢˜
    run_classification([class_1, class_2, class_3], [1, 2, 3], [1/3, 1/3, 1/3]) # ä¼ å…¥ä¸‰ç±»æ•°æ®ï¼Œ3ä¸ªç»´åº¦ç‰¹å¾ï¼Œå…ˆéªŒæ¦‚ç‡ä¸º1/3

if __name__ == "__main__":
    plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
    plt.rcParams['axes.unicode_minus'] = False    # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    main()